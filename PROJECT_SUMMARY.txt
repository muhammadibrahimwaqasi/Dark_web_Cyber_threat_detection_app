# Project Summary: Darkweb Threat Analytics

A Django-based platform that blends text classification of cybersecurity-related content with optional AI-assisted analysis, exposing a REST API and a lightweight dashboard.

## Key Stack
- Python, Django, Django REST Framework
- Scikit-learn (TF-IDF + RandomForest) for local threat classification
- Joblib for model persistence
- Optional Gemini AI integration (Google Generative AI)
- SQLite database for lightweight persistence during development

## Repository Layout
- `manage.py` – Django CLI helper to run server, migrations, etc.
- `darkweb_app/` – Core Django project settings, WSGI/ASGI, and project-level config.
- `api/` – REST API app: models, views, serializers (where applicable), and routes.
- `dashboard/` – Lightweight dashboard app to render analytics pages.
- `data/` – Raw and cleaned datasets used for ML training and evaluation.
- `models/` – Trained ML model and vectorizer pickles generated by the training script.
- `scripts/` – Data cleaning and preprocessing utilities to produce cleaned datasets.
- `utils/` – Helper utilities (LLM wrappers, etc.).
- `static/` – Static assets and templates for dashboard UI.
- `.env` – Environment configuration (hidden in this environment; used by the app for secrets).
- `TRAIN_MODEL.py` – Script demonstrating how a model is trained and saved.

> Note: The app is currently configured for development use (DEBUG=True, SQLite). For production, you should configure a secure database, static file hosting, and proper secret management.

## Core Components
- Django Project: darkweb_app
  - Settings: `darkweb_app/settings.py` defines installed apps, middleware, database, static files, and template configuration.
  - URL routing: `darkweb_app/urls.py` routes the root, admin, and includes `api` and `dashboard` apps.
  - WSGI/ASGI: `darkweb_app/wsgi.py` and `darkweb_app/asgi.py` expose the entry points for hosting servers.
- API App: `api/`
  - Models: `api/models.py` defines `PredictionLog` for logging inputs and predictions.
  - Views: `api/views.py` implements endpoints for health, root, prediction, AI analysis, and stats.
  - URLs: `api/urls.py` exposes `/api/` endpoints such as `/predict/`, `/analyze/`, and `/stats/`.
- Dashboard App: `dashboard/`
  - Minimal view `dashboard_view` renders a basic dashboard template and is wired via `dashboard/urls.py`.
- Data & ML: `data/`, `models/`, `scripts/`, `train_model.py`
  - Cleaning scripts: `scripts/clean_*.py` generate cleaned CSVs from raw data.
  - Training: `train_model.py` reads cleaned datasets, labels data with a keyword-based heuristic, vectorizes with TF-IDF, trains a RandomForest, and saves model artifacts to `models/`.
  - Models: `models/cyber_model.pkl`, `models/vectorizer.pkl` consumed by the API for predictions.
- Utilities: `utils/llm_helper.py` provides an interface to an LLM (OpenAI) for optional threat analysis.

## Data Sources
- Agora data: `data/agora_clean.csv` (post-cleaning)
- Clydeiii data: `data/clydeiii_final_clean.csv` (post-cleaning)
- Global threats: `data/global_threats_clean.csv` (post-cleaning)
- MrMoor: `data/mrmoor_clean.csv` (post-cleaning)
- Original cleaning scripts convert raw CSVs/text dumps to these cleaned files (e.g., `scripts/clean_agora.py`, `scripts/clean_clydeiii_csv.py`, etc.).

## ML Pipeline (High Level)
- Data ingestion: concatenate cleaned CSVs found in `data/` (Agora, Clydeiii, MrMoor, Global Threats).
- Text extraction: detect the text column heuristically, default to the sole column if present.
- Labeling: assign binary labels based on a keyword list related to cyber threats.
- Vectorization: TF-IDF with up to 8k features and 1-3 n-grams (English stopwords).
- Model: RandomForestClassifier with class balancing; 300 trees; multi-threaded training.
- Evaluation: prints a classification report on a held-out test split.
- Persistence: saves `cyber_model.pkl` and `vectorizer.pkl` to `models/`.

## API Overview
- Base URL: `/api/` (root route provided by `api/urls.py`)
- Endpoints:
  - `GET /status/` - health: shows model load status and Gemini connectivity.
  - `POST /predict/` - simple local inference using the TF-IDF + RF model.
  - `POST /analyze/` - Gemini AI-backed analysis (optional; requires proper API key).
  - `GET /stats/` - aggregated prediction stats for dashboard.
- Data persistence: `api/models.PredictionLog` stores the input text, predicted label, and optional LLM analysis.

## Gemini AI Integration (Optional)
- Uses Google Generative AI (Gemini) if `GEMINI_API_KEY` is provided via environment/.env.
- Attempts to fetch a suitable model, then prompts the model to analyze the message and return a short verdict.
- Results are stored in `PredictionLog.llm_analysis`.

## Running Locally (Development)
1) Create a Python 3.11+ environment and install dependencies (e.g., Django, djangorestframework, scikit-learn, joblib, python-dotenv, google-generativeai).
2) Set up environment variables (e.g., `DJANGO_SETTINGS_MODULE=darkweb_app.settings`, `GEMINI_API_KEY`, `OPENAI_API_KEY` if used by utils).
3) Migrate DB: `python manage.py migrate`.
4) (Optional) Train model: `python train_model.py` (produces `data/*_clean.csv` and `models/cyber_model.pkl`, `models/vectorizer.pkl`).
5) Run server: `python manage.py runserver 8000` and navigate to `http://127.0.0.1:8000/api/` or `http://127.0.0.1:8000/dashboard/`.

Notes:
- DEBUG is enabled in development settings; switch to production configs for deployment.
- The `.env` file contains sensitive keys (Gemini, OpenAI). Do not commit or expose these.
- This repo uses SQLite by default (`db.sqlite3`). For production, point to a robust RDBMS and configure static hosting.

## Testing & Quality
- Minimal unit tests exist in `api/tests.py` placeholder; no extensive tests currently.
- The project relies on print statements for runtime feedback; consider integrating Python logging and pytest for robust testing.

## Known Gaps & Next Steps
- Add serializers for api/models if needed to evolve API responses.
- Replace keyword-based labeling with a more robust labeling/annotation approach.
- Improve dashboard with a real frontend using the existing static assets.
- Add CI/CD, linting, and tests.

## Key References (paths)
- Django project:
  - `manage.py`, `darkweb_app/settings.py`, `darkweb_app/urls.py`, `darkweb_app/wsgi.py`, `darkweb_app/asgi.py`
- API:
  - `api/views.py`, `api/urls.py`, `api/models.py`
- Dashboard:
  - `dashboard/views.py`, `dashboard/urls.py`
- ML & Data:
  - `train_model.py`, `scripts/clean_*.py`, `data/*.csv`, `models/cyber_model.pkl`, `models/vectorizer.pkl`
- Utilities:
  - `utils/llm_helper.py`
